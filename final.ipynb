{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1da4Z8RFylbD8Vj_J409vx-C-8c_m1Zix","authorship_tag":"ABX9TyM8cE4rDjcm6YWEjDXrD5C4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# <font color=\"#1b5776\">1. <u>Adding Imports</u></font>"],"metadata":{"id":"-MZ8QkJfUlgf"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"uVBUQbD3SaOP","executionInfo":{"status":"ok","timestamp":1643349504506,"user_tz":-330,"elapsed":443,"user":{"displayName":"Toushali Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIC7-F4DTV-HOcWx-MgJmPHg9g8HXHDosR87ZXgO0=s64","userId":"01327586349516820169"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","from datetime import datetime as dte\n","import time\n","from sklearn.metrics import mean_squared_error\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","source":["# <font color=\"#1b5776\">2. <u>Fetching relevant files for Pipeline Creation Pipeline Creation</u></font>"],"metadata":{"id":"I3T3ak-HQTXT"}},{"cell_type":"code","source":["def get_relevant_files():\n","  \"\"\"fetching data relevant to cleaning, imputation, pre-processing & featurization\"\"\"\n","\n","  def get_file(f_path):\n","    \"\"\"Takes path & returns the data present in that file\"\"\"\n","    data = pickle.load( open(f_path,\"rb\") )\n","    return data\n","\n","  f_path = \"/content/drive/MyDrive/AAIC - Assignments/SNo.23_Self Case Study 1/Output_Files/drop_columns.pkl\"\n","  drop_columns = get_file(f_path)\n","  f_path = \"/content/drive/MyDrive/AAIC - Assignments/SNo.23_Self Case Study 1/Output_Files/impute_columns_with_zero.pkl\"\n","  impute_columns_with_zero = get_file(f_path)\n","  f_path = \"/content/drive/MyDrive/AAIC - Assignments/SNo.23_Self Case Study 1/Output_Files/impute_columns_with_OTHERS.pkl\"\n","  impute_columns_with_OTHERS = get_file(f_path)\n","  f_path = \"/content/drive/MyDrive/AAIC - Assignments/SNo.23_Self Case Study 1/Output_Files/to_replace_dict.pkl\"\n","  to_replace_dict = get_file(f_path)\n","  f_path = \"/content/drive/MyDrive/AAIC - Assignments/SNo.23_Self Case Study 1/Output_Files/train_columns_sequence.pkl\"\n","  train_columns_sequence = get_file(f_path)\n","  f_path = \"/content/drive/MyDrive/AAIC - Assignments/SNo.23_Self Case Study 1/Output_Files/CUSTOM_ENCODING_DICT.pkl\"\n","  CUSTOM_ENCODING_DICT = get_file(f_path)\n","  f_path = \"/content/drive/MyDrive/AAIC - Assignments/SNo.23_Self Case Study 1/Output_Files/set1.pkl\"\n","  set1 = get_file(f_path)\n","  f_path = \"/content/drive/MyDrive/AAIC - Assignments/SNo.23_Self Case Study 1/Output_Files/set2.pkl\"\n","  set2 = get_file(f_path)\n","  f_path = \"/content/drive/MyDrive/AAIC - Assignments/SNo.23_Self Case Study 1/Output_Files/set3.pkl\"\n","  set3 = get_file(f_path)\n","  f_path = \"/content/drive/MyDrive/AAIC - Assignments/SNo.23_Self Case Study 1/Output_Files/set4.pkl\"\n","  set4 = get_file(f_path)\n","  f_path = \"/content/drive/MyDrive/AAIC - Assignments/SNo.23_Self Case Study 1/Output_Files/set5.pkl\"\n","  set5 = get_file(f_path)\n","  f_path = \"/content/drive/MyDrive/AAIC - Assignments/SNo.23_Self Case Study 1/Output_Files/pre_gap_dates.pkl\"\n","  pre_gap_dates = get_file(f_path)\n","  f_path = \"/content/drive/MyDrive/AAIC - Assignments/SNo.23_Self Case Study 1/Output_Files/final_test_data_type_dict.pkl\"\n","  final_test_data_type_dict = get_file(f_path)\n","  f_path = \"/content/drive/MyDrive/AAIC - Assignments/SNo.23_Self Case Study 1/Output_Files_2/models_4.1/gbdt_cls_model.sav\"\n","  loaded_model_cls = get_file(f_path)\n","  f_path = \"/content/drive/MyDrive/AAIC - Assignments/SNo.23_Self Case Study 1/Output_Files_2/models_4.1/gbdt_reg_model.sav\"\n","  loaded_model_reg = get_file(f_path)\n","\n","  return drop_columns,impute_columns_with_zero,impute_columns_with_OTHERS,to_replace_dict,train_columns_sequence,CUSTOM_ENCODING_DICT,\\\n","  set1,set2,set3,set4,set5,pre_gap_dates,final_test_data_type_dict,loaded_model_cls,loaded_model_reg\n","\n","drop_columns,impute_columns_with_zero,impute_columns_with_OTHERS,to_replace_dict,train_columns_sequence,CUSTOM_ENCODING_DICT,\\\n","set1,set2,set3,set4,set5,pre_gap_dates,final_test_data_type_dict,loaded_model_cls,loaded_model_reg=get_relevant_files()"],"metadata":{"id":"4zowGGQwQZCy","executionInfo":{"status":"ok","timestamp":1643349728965,"user_tz":-330,"elapsed":9,"user":{"displayName":"Toushali Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIC7-F4DTV-HOcWx-MgJmPHg9g8HXHDosR87ZXgO0=s64","userId":"01327586349516820169"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# <font color=\"#1b5776\">3. <u>Prediction: Pipeline Creation</u></font>"],"metadata":{"id":"ShwdytPpv_0y"}},{"cell_type":"code","source":["def final(X):\n","  \"\"\"Creates entire pipeline for predicting revenue for a test data point and returns the predicted revenue(s)\"\"\"\n","  \n","  # MAKING TEST COLUMNS SEQUENCE THE SAME AS IT WAS IN TRAIN.\n","  X = X[train_columns_sequence]\n","  #-----------------------------------------------------------------------------\n","  # DATA CLEANING.\n","  for col in X.columns:\n","    if col in drop_columns:\n","      X.drop(columns=[col],axis=1,inplace=True)\n","    elif col in impute_columns_with_zero:\n","      X[col].fillna(value=0,inplace=True)\n","    elif col in impute_columns_with_OTHERS:\n","      X[col].fillna(value=\"OTHERS\",inplace=True)\n","      X[col] = X[col].replace(to_replace=to_replace_dict)\n","  #-----------------------------------------------------------------------------\n","  # CONVERTING DATA COLUMN INTO DATETIME OBJECT.\n","  X['date'] = pd.to_datetime(arg=X['date'], format=\"%Y%m%d\")\n","  #-----------------------------------------------------------------------------\n","  # EXTRACTING USEFUL FEATURES FROM DATE.\n","  X[\"year\"] = X[\"date\"].dt.year\n","  X[\"month\"] = X[\"date\"].dt.month\n","  X[\"dayOfMonth\"] = X[\"date\"].dt.day\n","  X[\"dayOfWeek\"] = X[\"date\"].dt.dayofweek\n","  X[\"dayName\"] = X[\"date\"].dt.day_name()\n","  X[\"weekOfYear\"] = X[\"date\"].dt.isocalendar().week\n","  X[\"dayOfYear\"] = X[\"date\"].dt.dayofyear\n","  X[\"quarter\"] = X[\"date\"].dt.quarter\n","  X[\"dayOfYear\"] = X[\"date\"].dt.dayofyear\n","  #-----------------------------------------------------------------------------\n","  # DESIGN NEW FEATURES USING DATE.\n","  def detectYearEnd(x):\n","    \"\"\"Takes in date and detects whether it is year end (Oct-Dec) or not\"\"\"\n","    if x.month in [10,11,12]:\n","      return \"Yes\"\n","    else:\n","      return \"No\"\n","  X[\"isYearEnd\"] = X[\"date\"].apply(detectYearEnd)\n","  #-----------------------------------------------------------------------------\n","  # DESIGN NEW FEATURES USING DATE.\n","  def check_weekend(x):\n","    \"\"\"Takes in a day name and detects whether it is weekend or not\"\"\"\n","    if x in [\"Saturday\",\"Sunday\"]:\n","      return \"Yes\"\n","    else:\n","      return \"No\"\n","  X[\"is_weekend\"] = X[\"dayName\"].apply(check_weekend)\n","  #-----------------------------------------------------------------------------\n","  # DESIGN NEW FEATURES USING VISIT START TIME.\n","  def getFeaures_fromPOSIXtimestamp(x):\n","    \"\"\"Takes time and returns timestamp in current format.\"\"\"\n","    return dte.fromtimestamp(x)\n","  def getHr_fromPOSIX(x):\n","    \"\"\"Returns hour from timestamp\"\"\"\n","    return x.hour\n","  def getMin_fromPOSIX(x):\n","    \"\"\"Returns minute from timestamp\"\"\"\n","    return x.minute\n","  def getSec_fromPOSIX(x):\n","    \"\"\"Returns second from timestamp\"\"\"\n","    return x.second\n","  def createFeaturesFromVisitStartTime(df):\n","    \"\"\"Returns the dataframe with newly formed features using visitStartTime\"\"\"\n","    whole_timestamp = df[\"visitStartTime\"].apply(getFeaures_fromPOSIXtimestamp)\n","    df[\"visit_hr\"] = whole_timestamp.apply(getHr_fromPOSIX)\n","    df[\"visit_min\"] = whole_timestamp.apply(getMin_fromPOSIX)\n","    df[\"visit_sec\"] = whole_timestamp.apply(getSec_fromPOSIX)\n","    return df\n","  X = createFeaturesFromVisitStartTime(X)\n","  #-----------------------------------------------------------------------------\n","  # DROPPING COLUMN NO LONGER NEEDED.\n","  X.drop(columns=[\"visitStartTime\"],axis=1,inplace=True)\n","  #-----------------------------------------------------------------------------\n","  # DESIGN NEW FEATURES USING TIME.\n","  def return_TimeOfDay(x):\n","    \"\"\"Takes in hour of the day and detects time of day (midnight/morning/afternoon/evening/night)\"\"\"\n","    if x<6:\n","      return \"midnight(12am-6am)\"\n","    elif x>=6 and x<12:\n","      return \"morning(6am-12pm)\"\n","    elif x>=12 and x<18:\n","      return \"afternoon_evening(12pm-6pm)\"\n","    else:\n","      return \"night(6pm-12am)\"\n","  X[\"time_of_day\"] = X[\"visit_hr\"].apply(return_TimeOfDay)\n","  #-----------------------------------------------------------------------------\n","  # DESIGN NEW FEATURES USING TIME.\n","  def detect_office_hours(x):\n","    \"\"\"Takes in hour of day; returns 'Yes' if it is office hours (i.e. 9am to 5pm) else returns 'No'\"\"\"\n","    if x>=9 and x<=17:\n","      return \"Yes\"\n","    else:\n","      return \"No\"\n","  X[\"is_office_hours\"] = X[\"visit_hr\"].apply(detect_office_hours)\n","  #-----------------------------------------------------------------------------\n","  # PROCESSING A BOOLEAN FEATURE.\n","  def detectMobile(x):\n","    \"\"\"Returns 'Yes' if it is mobile, else 'No'\"\"\"\n","    if x==True:\n","      return \"Yes\"\n","    else:\n","      return \"No\"\n","  X['device_isMobile'] = X['device_isMobile'].apply(detectMobile)\n","  #-----------------------------------------------------------------------------\n","  # DROPPING COLUMN NO LONGER NEEDED.\n","  X.drop(columns=[\"dayName\"],axis=1,inplace=True)\n","  #-----------------------------------------------------------------------------\n","  # PROCESSING YEAR COLUMN.\n","  X['year'] = X['year'].astype(\"object\")\n","  #-----------------------------------------------------------------------------\n","  # CUSTOM CATEGORICAL ENCODING.\n","  def perform_custom_encoding(x,feature_specfic_dict):\n","    \"\"\"This is like transform method. It will take a value -->x and encode it using the feature-specific dictionary passed here.\"\"\"\n","    if x in feature_specfic_dict.keys():\n","      return feature_specfic_dict[x]\n","    else:\n","      return 0 # this 0 is for those feature values which wasn't seen during the fit method in train dataset\n","  for feature in CUSTOM_ENCODING_DICT.keys():\n","    X[feature] = X[feature].apply(perform_custom_encoding, args=(CUSTOM_ENCODING_DICT[feature],))\n","  #-----------------------------------------------------------------------------\n","  # GETTING VISITOR-LEVEL DATA VIA HELPER FUNCTIONS.\n","  def aggApply_ModeMax(x):\n","    \"\"\"\n","    eg-1: For browser feature:if customer visited 3 times via Chrome and never via any other browser, then return 3.\n","    eg-2: For weekend feature: if customer visited 2 times on weekdays and 2 times on weekends, i.e. we'll have two 1s and two 0s --> then return 1.\n","          Max is taken for this kind of case just to select 1 value. Note: Taking max/min won't affect the actual mode obtained.\n","    \"\"\"\n","    return x.mode().max()\n","  def agg_apply_Median(x):\n","    \"\"\"return median of all values\"\"\"\n","    return x.median()\n","  def agg_apply_Mean(x):\n","    \"\"\"return mean of all values\"\"\"\n","    return x.mean()\n","  def agg_apply_Sum(x):\n","    \"\"\"return sum of all values\"\"\"\n","    return x.mean()\n","  def agg_apply_Min(x):\n","    \"\"\"return minimum of all values\"\"\"\n","    return x.min()\n","  def agg_apply_Max(x):  \n","    \"\"\"return max of all values\"\"\"\n","    return x.max()\n","  def agg_apply_LogOfSum(x):\n","    \"\"\"return log of (sum of all values + 1). NOTE: adding 1 just to handle 0 sum.\"\"\"\n","    return np.log1p(np.sum(x))\n","  def agg_apply_SpanVisits(x):\n","    \"\"\"returns no. of days in between 1st visit & last visit in this particular aggregated time-frame\"\"\"\n","    return (x.max() - x.min()).days\n","  def get_apply_dict(df):\n","    \"\"\"Returns a dictionary with feature name and list of tuples of new aggregated feature name & aggregation functions to apply.\"\"\"\n","    apply_dict = {}\n","    for col in df.columns:\n","      if (col in set1) or (col in set2):\n","        new_feature_name = col+\"_ModeMax\"\n","        apply_dict[col] = [(new_feature_name, aggApply_ModeMax)]\n","      if col in set3:\n","        new_feature_name1 = col+\"_Median\"\n","        new_feature_name2 = col+\"_Mean\"\n","        new_feature_name3 = col+\"_Sum\"\n","        new_feature_name4 = col+\"_Min\"\n","        new_feature_name5 = col+\"_Max\"\n","        apply_list = [(new_feature_name1, agg_apply_Median), (new_feature_name2, agg_apply_Mean),\n","                      (new_feature_name3, agg_apply_Sum), (new_feature_name4, agg_apply_Min),\n","                      (new_feature_name5, agg_apply_Max)]\n","        if col in apply_dict.keys(): # checking if this col is already present in the dict\n","          apply_dict[col].extend(apply_list)\n","        else:\n","          apply_dict[col] = apply_list\n","      if col in set4:\n","        new_feature_name1 = col+\"_Span\"\n","        new_feature_name2 = col+\"_FirstVisit\"\n","        new_feature_name3 = col+\"_LastVisit\"\n","        apply_dict[col] = [(new_feature_name1, agg_apply_SpanVisits),(new_feature_name2, \"min\"),\n","                          (new_feature_name3, \"max\")]\n","      if col in set5:\n","        new_feature_name = col+\"_LogOfSum\"\n","        apply_dict[col] = [(new_feature_name, agg_apply_LogOfSum)]\n","    return apply_dict\n","  def vistor_level_data(data):\n","    data = data.groupby(by=\"fullVisitorId\").agg(get_apply_dict(data))\n","    return data\n","  X = vistor_level_data(X)\n","  # dropping the multi-index level, resetting index and set fullVisitorId as the 1st column.\n","  X.columns = X.columns.droplevel()\n","  X = X.reset_index()\n","  #-----------------------------------------------------------------------------\n","  # MORE NEW FEATURES.\n","  def firstVisitAfterStart(x,start):\n","    \"\"\"No. of days after current period's start date, the First visit occurred\"\"\"\n","    return (x - start).days\n","  def lastVisitBeforeEnd(x,end):\n","    \"\"\"No. of days before current period's end date, the Last visit occurred\"\"\"\n","    return (end - x).days\n","  def apply_to_splits(split_df,current_period_start,current_period_end):\n","    \"\"\"Applies function to 2 date columns and returns the modified dataframe\"\"\"\n","    split_df[\"firstVisit_AfterStart\"] = split_df[\"date_FirstVisit\"].apply(firstVisitAfterStart, args=(current_period_start,))\n","    split_df[\"lastVisit_BeforeEnd\"] = split_df[\"date_LastVisit\"].apply(lastVisitBeforeEnd, args=(current_period_end,))\n","    split_df = split_df.drop(columns=[\"date_FirstVisit\",\"date_LastVisit\"], axis=1)\n","    return split_df\n","  X = apply_to_splits(X, pre_gap_dates[0], pre_gap_dates[1])\n","  #-----------------------------------------------------------------------------\n","  # MAKING SURE DATA TYPES ARE SAME AS IT WAS WHILE TRAINING.\n","  X = X.astype(final_test_data_type_dict)\n","  #-----------------------------------------------------------------------------\n","  #PREDICTIONS USING SAVED MODELS.\n","  test_columnsToDrop = [\"fullVisitorId\",\"totals_transactionRevenue_LogOfSum\"]\n","  X = X.drop(columns=test_columnsToDrop,axis=1)\n","  cls_prediction = loaded_model_cls.predict_proba(X)[:,1]\n","  reg_prediction = loaded_model_reg.predict(X)\n","  final_prediction = cls_prediction*reg_prediction\n","  return final_prediction"],"metadata":{"id":"sKoLF8SiGnlZ","executionInfo":{"status":"ok","timestamp":1643349775620,"user_tz":-330,"elapsed":461,"user":{"displayName":"Toushali Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIC7-F4DTV-HOcWx-MgJmPHg9g8HXHDosR87ZXgO0=s64","userId":"01327586349516820169"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# <font color=\"#1b5776\">4. <u>Prediction</u></font>"],"metadata":{"id":"-KI0582bwQ-J"}},{"cell_type":"code","source":["# reading the preliminarily processed test file:-\n","test = pd.read_csv(\"/content/drive/MyDrive/AAIC - Assignments/SNo.23_Self Case Study 1/Output_Files/test_cleaned2.csv\",dtype={\"fullVisitorId\":\"string\"})"],"metadata":{"id":"hTThn5h9bTnf","executionInfo":{"status":"ok","timestamp":1643349783507,"user_tz":-330,"elapsed":6088,"user":{"displayName":"Toushali Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIC7-F4DTV-HOcWx-MgJmPHg9g8HXHDosR87ZXgO0=s64","userId":"01327586349516820169"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["q = test.iloc[[1]].copy()\n","print(\"Query point shape:\",q.shape)"],"metadata":{"id":"mLa_xybsH8uP","executionInfo":{"status":"ok","timestamp":1643349783508,"user_tz":-330,"elapsed":14,"user":{"displayName":"Toushali Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIC7-F4DTV-HOcWx-MgJmPHg9g8HXHDosR87ZXgO0=s64","userId":"01327586349516820169"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8b81dd4a-b137-4ba9-b5f8-5fd339d91517"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Query point shape: (1, 59)\n"]}]},{"cell_type":"code","source":["start = time.time()\n","prediction = final(q)\n","end = time.time()\n","time_taken = end-start\n","print(\"Predicted revenue(s):\\n\",prediction)\n","print(\"Time Taken for Prediction:\",round(time_taken,4),\"seconds or\",round(time_taken*1000,4),\"milli-seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gGlwhgedR8MR","executionInfo":{"status":"ok","timestamp":1643349810713,"user_tz":-330,"elapsed":444,"user":{"displayName":"Toushali Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIC7-F4DTV-HOcWx-MgJmPHg9g8HXHDosR87ZXgO0=s64","userId":"01327586349516820169"}},"outputId":"1742b779-fd43-4c2a-f1e9-ba384fe39bf2"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted revenue(s):\n"," [0.00479661]\n","Time Taken for Prediction: 0.196 seconds or 196.0394 milli-seconds\n"]}]},{"cell_type":"markdown","source":["# <font color=\"#1b5776\">5. <u>Prediction & Obtaining Metric Value: Pipeline Creation</u></font>"],"metadata":{"id":"jVDdQcZoVxqT"}},{"cell_type":"code","source":["def final2(X):\n","  \"\"\"Creates entire pipeline for predicting revenue for a test data point and returns the predicted revenue(s)\"\"\"\n","  \n","  # MAKING TEST COLUMNS SEQUENCE THE SAME AS IT WAS IN TRAIN.\n","  X = X[train_columns_sequence]\n","  #-----------------------------------------------------------------------------\n","  # DATA CLEANING.\n","  for col in X.columns:\n","    if col in drop_columns:\n","      X.drop(columns=[col],axis=1,inplace=True)\n","    elif col in impute_columns_with_zero:\n","      X[col].fillna(value=0,inplace=True)\n","    elif col in impute_columns_with_OTHERS:\n","      X[col].fillna(value=\"OTHERS\",inplace=True)\n","      X[col] = X[col].replace(to_replace=to_replace_dict)\n","  #-----------------------------------------------------------------------------\n","  # CONVERTING DATA COLUMN INTO DATETIME OBJECT.\n","  X['date'] = pd.to_datetime(arg=X['date'], format=\"%Y%m%d\")\n","  #-----------------------------------------------------------------------------\n","  # EXTRACTING USEFUL FEATURES FROM DATE.\n","  X[\"year\"] = X[\"date\"].dt.year\n","  X[\"month\"] = X[\"date\"].dt.month\n","  X[\"dayOfMonth\"] = X[\"date\"].dt.day\n","  X[\"dayOfWeek\"] = X[\"date\"].dt.dayofweek\n","  X[\"dayName\"] = X[\"date\"].dt.day_name()\n","  X[\"weekOfYear\"] = X[\"date\"].dt.isocalendar().week\n","  X[\"dayOfYear\"] = X[\"date\"].dt.dayofyear\n","  X[\"quarter\"] = X[\"date\"].dt.quarter\n","  X[\"dayOfYear\"] = X[\"date\"].dt.dayofyear\n","  #-----------------------------------------------------------------------------\n","  # DESIGN NEW FEATURES USING DATE.\n","  def detectYearEnd(x):\n","    \"\"\"Takes in date and detects whether it is year end (Oct-Dec) or not\"\"\"\n","    if x.month in [10,11,12]:\n","      return \"Yes\"\n","    else:\n","      return \"No\"\n","  X[\"isYearEnd\"] = X[\"date\"].apply(detectYearEnd)\n","  #-----------------------------------------------------------------------------\n","  # DESIGN NEW FEATURES USING DATE.\n","  def check_weekend(x):\n","    \"\"\"Takes in a day name and detects whether it is weekend or not\"\"\"\n","    if x in [\"Saturday\",\"Sunday\"]:\n","      return \"Yes\"\n","    else:\n","      return \"No\"\n","  X[\"is_weekend\"] = X[\"dayName\"].apply(check_weekend)\n","  #-----------------------------------------------------------------------------\n","  # DESIGN NEW FEATURES USING VISIT START TIME.\n","  def getFeaures_fromPOSIXtimestamp(x):\n","    \"\"\"Takes time and returns timestamp in current format.\"\"\"\n","    return dte.fromtimestamp(x)\n","  def getHr_fromPOSIX(x):\n","    \"\"\"Returns hour from timestamp\"\"\"\n","    return x.hour\n","  def getMin_fromPOSIX(x):\n","    \"\"\"Returns minute from timestamp\"\"\"\n","    return x.minute\n","  def getSec_fromPOSIX(x):\n","    \"\"\"Returns second from timestamp\"\"\"\n","    return x.second\n","  def createFeaturesFromVisitStartTime(df):\n","    \"\"\"Returns the dataframe with newly formed features using visitStartTime\"\"\"\n","    whole_timestamp = df[\"visitStartTime\"].apply(getFeaures_fromPOSIXtimestamp)\n","    df[\"visit_hr\"] = whole_timestamp.apply(getHr_fromPOSIX)\n","    df[\"visit_min\"] = whole_timestamp.apply(getMin_fromPOSIX)\n","    df[\"visit_sec\"] = whole_timestamp.apply(getSec_fromPOSIX)\n","    return df\n","  X = createFeaturesFromVisitStartTime(X)\n","  #-----------------------------------------------------------------------------\n","  # DROPPING COLUMN NO LONGER NEEDED.\n","  X.drop(columns=[\"visitStartTime\"],axis=1,inplace=True)\n","  #-----------------------------------------------------------------------------\n","  # DESIGN NEW FEATURES USING TIME.\n","  def return_TimeOfDay(x):\n","    \"\"\"Takes in hour of the day and detects time of day (midnight/morning/afternoon/evening/night)\"\"\"\n","    if x<6:\n","      return \"midnight(12am-6am)\"\n","    elif x>=6 and x<12:\n","      return \"morning(6am-12pm)\"\n","    elif x>=12 and x<18:\n","      return \"afternoon_evening(12pm-6pm)\"\n","    else:\n","      return \"night(6pm-12am)\"\n","  X[\"time_of_day\"] = X[\"visit_hr\"].apply(return_TimeOfDay)\n","  #-----------------------------------------------------------------------------\n","  # DESIGN NEW FEATURES USING TIME.\n","  def detect_office_hours(x):\n","    \"\"\"Takes in hour of day; returns 'Yes' if it is office hours (i.e. 9am to 5pm) else returns 'No'\"\"\"\n","    if x>=9 and x<=17:\n","      return \"Yes\"\n","    else:\n","      return \"No\"\n","  X[\"is_office_hours\"] = X[\"visit_hr\"].apply(detect_office_hours)\n","  #-----------------------------------------------------------------------------\n","  # PROCESSING A BOOLEAN FEATURE.\n","  def detectMobile(x):\n","    \"\"\"Returns 'Yes' if it is mobile, else 'No'\"\"\"\n","    if x==True:\n","      return \"Yes\"\n","    else:\n","      return \"No\"\n","  X['device_isMobile'] = X['device_isMobile'].apply(detectMobile)\n","  #-----------------------------------------------------------------------------\n","  # DROPPING COLUMN NO LONGER NEEDED.\n","  X.drop(columns=[\"dayName\"],axis=1,inplace=True)\n","  #-----------------------------------------------------------------------------\n","  # PROCESSING YEAR COLUMN.\n","  X['year'] = X['year'].astype(\"object\")\n","  #-----------------------------------------------------------------------------\n","  # CUSTOM CATEGORICAL ENCODING.\n","  def perform_custom_encoding(x,feature_specfic_dict):\n","    \"\"\"This is like transform method. It will take a value -->x and encode it using the feature-specific dictionary passed here.\"\"\"\n","    if x in feature_specfic_dict.keys():\n","      return feature_specfic_dict[x]\n","    else:\n","      return 0 # this 0 is for those feature values which wasn't seen during the fit method in train dataset\n","  for feature in CUSTOM_ENCODING_DICT.keys():\n","    X[feature] = X[feature].apply(perform_custom_encoding, args=(CUSTOM_ENCODING_DICT[feature],))\n","  #-----------------------------------------------------------------------------\n","  # GETTING VISITOR-LEVEL DATA VIA HELPER FUNCTIONS.\n","  def aggApply_ModeMax(x):\n","    \"\"\"\n","    eg-1: For browser feature:if customer visited 3 times via Chrome and never via any other browser, then return 3.\n","    eg-2: For weekend feature: if customer visited 2 times on weekdays and 2 times on weekends, i.e. we'll have two 1s and two 0s --> then return 1.\n","          Max is taken for this kind of case just to select 1 value. Note: Taking max/min won't affect the actual mode obtained.\n","    \"\"\"\n","    return x.mode().max()\n","  def agg_apply_Median(x):\n","    \"\"\"return median of all values\"\"\"\n","    return x.median()\n","  def agg_apply_Mean(x):\n","    \"\"\"return mean of all values\"\"\"\n","    return x.mean()\n","  def agg_apply_Sum(x):\n","    \"\"\"return sum of all values\"\"\"\n","    return x.mean()\n","  def agg_apply_Min(x):\n","    \"\"\"return minimum of all values\"\"\"\n","    return x.min()\n","  def agg_apply_Max(x):  \n","    \"\"\"return max of all values\"\"\"\n","    return x.max()\n","  def agg_apply_LogOfSum(x):\n","    \"\"\"return log of (sum of all values + 1). NOTE: adding 1 just to handle 0 sum.\"\"\"\n","    return np.log1p(np.sum(x))\n","  def agg_apply_SpanVisits(x):\n","    \"\"\"returns no. of days in between 1st visit & last visit in this particular aggregated time-frame\"\"\"\n","    return (x.max() - x.min()).days\n","  def get_apply_dict(df):\n","    \"\"\"Returns a dictionary with feature name and list of tuples of new aggregated feature name & aggregation functions to apply.\"\"\"\n","    apply_dict = {}\n","    for col in df.columns:\n","      if (col in set1) or (col in set2):\n","        new_feature_name = col+\"_ModeMax\"\n","        apply_dict[col] = [(new_feature_name, aggApply_ModeMax)]\n","      if col in set3:\n","        new_feature_name1 = col+\"_Median\"\n","        new_feature_name2 = col+\"_Mean\"\n","        new_feature_name3 = col+\"_Sum\"\n","        new_feature_name4 = col+\"_Min\"\n","        new_feature_name5 = col+\"_Max\"\n","        apply_list = [(new_feature_name1, agg_apply_Median), (new_feature_name2, agg_apply_Mean),\n","                      (new_feature_name3, agg_apply_Sum), (new_feature_name4, agg_apply_Min),\n","                      (new_feature_name5, agg_apply_Max)]\n","        if col in apply_dict.keys(): # checking if this col is already present in the dict\n","          apply_dict[col].extend(apply_list)\n","        else:\n","          apply_dict[col] = apply_list\n","      if col in set4:\n","        new_feature_name1 = col+\"_Span\"\n","        new_feature_name2 = col+\"_FirstVisit\"\n","        new_feature_name3 = col+\"_LastVisit\"\n","        apply_dict[col] = [(new_feature_name1, agg_apply_SpanVisits),(new_feature_name2, \"min\"),\n","                          (new_feature_name3, \"max\")]\n","      if col in set5:\n","        new_feature_name = col+\"_LogOfSum\"\n","        apply_dict[col] = [(new_feature_name, agg_apply_LogOfSum)]\n","    return apply_dict\n","  def vistor_level_data(data):\n","    data = data.groupby(by=\"fullVisitorId\").agg(get_apply_dict(data))\n","    return data\n","  X = vistor_level_data(X)\n","  # dropping the multi-index level, resetting index and set fullVisitorId as the 1st column.\n","  X.columns = X.columns.droplevel()\n","  X = X.reset_index()\n","  #-----------------------------------------------------------------------------\n","  # MORE NEW FEATURES.\n","  def firstVisitAfterStart(x,start):\n","    \"\"\"No. of days after current period's start date, the First visit occurred\"\"\"\n","    return (x - start).days\n","  def lastVisitBeforeEnd(x,end):\n","    \"\"\"No. of days before current period's end date, the Last visit occurred\"\"\"\n","    return (end - x).days\n","  def apply_to_splits(split_df,current_period_start,current_period_end):\n","    \"\"\"Applies function to 2 date columns and returns the modified dataframe\"\"\"\n","    split_df[\"firstVisit_AfterStart\"] = split_df[\"date_FirstVisit\"].apply(firstVisitAfterStart, args=(current_period_start,))\n","    split_df[\"lastVisit_BeforeEnd\"] = split_df[\"date_LastVisit\"].apply(lastVisitBeforeEnd, args=(current_period_end,))\n","    split_df = split_df.drop(columns=[\"date_FirstVisit\",\"date_LastVisit\"], axis=1)\n","    return split_df\n","  X = apply_to_splits(X, pre_gap_dates[0], pre_gap_dates[1])\n","  #-----------------------------------------------------------------------------\n","  # MAKING SURE DATA TYPES ARE SAME AS IT WAS WHILE TRAINING.\n","  X = X.astype(final_test_data_type_dict)\n","  #-----------------------------------------------------------------------------\n","  #PREDICTIONS USING SAVED MODELS.\n","  test_columnsToDrop = [\"fullVisitorId\",\"totals_transactionRevenue_LogOfSum\"]\n","  Y = X['totals_transactionRevenue_LogOfSum']\n","  X = X.drop(columns=test_columnsToDrop,axis=1)\n","  cls_prediction = loaded_model_cls.predict_proba(X)[:,1]\n","  reg_prediction = loaded_model_reg.predict(X)\n","  final_prediction = cls_prediction*reg_prediction\n","  rmse_metric_val = mean_squared_error(y_true=Y,y_pred=final_prediction,squared=False)\n","  return final_prediction,rmse_metric_val"],"metadata":{"id":"IhPVoq39TfiV","executionInfo":{"status":"ok","timestamp":1643349822018,"user_tz":-330,"elapsed":442,"user":{"displayName":"Toushali Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIC7-F4DTV-HOcWx-MgJmPHg9g8HXHDosR87ZXgO0=s64","userId":"01327586349516820169"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# <font color=\"#1b5776\">6. <u>Prediction + Metric</u></font>"],"metadata":{"id":"TTVCCzdMWKEM"}},{"cell_type":"code","source":["# reading the preliminarily processed test file:-\n","test = pd.read_csv(\"/content/drive/MyDrive/AAIC - Assignments/SNo.23_Self Case Study 1/Output_Files/test_cleaned2.csv\",dtype={\"fullVisitorId\":\"string\"})"],"metadata":{"id":"EVxrmgILWKEn","executionInfo":{"status":"ok","timestamp":1643349222639,"user_tz":-330,"elapsed":7616,"user":{"displayName":"Toushali Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIC7-F4DTV-HOcWx-MgJmPHg9g8HXHDosR87ZXgO0=s64","userId":"01327586349516820169"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["q = test.iloc[[1]].copy()\n","print(\"Query point shape:\",q.shape)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1643349224378,"user_tz":-330,"elapsed":11,"user":{"displayName":"Toushali Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIC7-F4DTV-HOcWx-MgJmPHg9g8HXHDosR87ZXgO0=s64","userId":"01327586349516820169"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"93dcb5a7-8df4-4a26-b3ad-bf674cbae6eb","id":"Kbwiw9FaWKEo"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Query point shape: (1, 59)\n"]}]},{"cell_type":"code","source":["start = time.time()\n","prediction,rmse_val = final2(q)\n","end = time.time()\n","time_taken = end-start\n","print(\"Predicted revenue(s):\\n\",prediction)\n","print(\"RMSE metric value:\\n\",rmse_val)\n","print(\"Time Taken for Prediction:\",round(time_taken,4),\"seconds or\",round(time_taken*1000,4),\"milli-seconds\")"],"metadata":{"executionInfo":{"status":"ok","timestamp":1643349837339,"user_tz":-330,"elapsed":704,"user":{"displayName":"Toushali Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjIC7-F4DTV-HOcWx-MgJmPHg9g8HXHDosR87ZXgO0=s64","userId":"01327586349516820169"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"75e51cb2-ce0f-4468-df8e-c5882c815a8e","id":"kHfXlp8HWKEp"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted revenue(s):\n"," [0.00479661]\n","RMSE metric value:\n"," 0.004796612076461315\n","Time Taken for Prediction: 0.1945 seconds or 194.4728 milli-seconds\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"ReZC0PSKPWM8"},"execution_count":null,"outputs":[]}]}